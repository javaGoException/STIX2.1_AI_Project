{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c979988e-f3f1-41e3-b81b-eb61a3de5a9c",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "id": "b427a805-bcc8-419e-b038-cf30d7ebe8f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T13:13:25.111049Z",
     "start_time": "2025-06-29T13:13:23.172860Z"
    }
   },
   "source": [
    "import json\n",
    "import ast\n",
    "import os\n",
    "from os import getenv\n",
    "from stix2validator import validate_file, print_results\n",
    "from dotenv import load_dotenv\n",
    "from neo4j import GraphDatabase\n",
    "import csv\n",
    "import time\n",
    "\n",
    "from neo4j_graphrag.embeddings import OllamaEmbeddings\n",
    "from neo4j_graphrag.indexes import create_vector_index\n",
    "from neo4j_graphrag.indexes import upsert_vectors\n",
    "from neo4j_graphrag.types import EntityType\n",
    "from neo4j_graphrag.retrievers import VectorRetriever\n",
    "from neo4j_graphrag.llm import OllamaLLM\n",
    "\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "db_uri = getenv(\"db_uri\")\n",
    "db_name = getenv(\"db_name\")\n",
    "db_username = getenv(\"db_username\")\n",
    "db_password= getenv(\"db_password\")\n",
    "\n",
    "auth = (db_username, db_password)\n",
    "driver = GraphDatabase.driver(uri=db_uri, auth=auth)\n",
    "embedder = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "llm = OllamaLLM(model_name=\"deepseek-r1:1.5b\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "934d2c97-9121-4d66-928d-ec25516a86f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:29:33.505882Z",
     "start_time": "2025-06-28T20:25:01.151873Z"
    }
   },
   "source": [
    "#main function to load SDOs\n",
    "def load_sdos(path):\n",
    "    with open(path) as f:\n",
    "        stix_json_data = json.load(f)\n",
    "\n",
    "    stix_objects = [obj for obj in stix_json_data[\"objects\"] if obj[\"type\"] not in (\"relationship\", \"x-mitre-collection\")]\n",
    "\n",
    "    for stix_object in stix_objects:\n",
    "\n",
    "        label = to_pascal_case(stix_object[\"type\"])\n",
    "        object_properties = get_stix_properties_dict(stix_object)\n",
    "\n",
    "        query = f\"\"\"\n",
    "            MERGE (x:SDO:{label} {{id: \"{stix_object[\"id\"]}\"}})\n",
    "            SET x = $properties\n",
    "        \"\"\"\n",
    "\n",
    "        session.run(query, properties=object_properties)\n",
    "\n",
    "\n",
    "#main function to load SROs\n",
    "def load_sros(path):\n",
    "    with open(path) as f:\n",
    "        stix_json_data = json.load(f)\n",
    "\n",
    "    stix_relationships = [rel for rel in stix_json_data[\"objects\"] if rel[\"type\"] in \"relationship\"]\n",
    "\n",
    "    for stix_relationship in stix_relationships:\n",
    "\n",
    "        relationship_name = to_pascal_case(stix_relationship[\"relationship_type\"])\n",
    "        relationship_properties = get_stix_properties_dict(stix_relationship)\n",
    "\n",
    "        query = f\"\"\"\n",
    "            MATCH (sourceObject {{id: \"{stix_relationship[\"source_ref\"]}\"}}), (targetObject {{id: \"{stix_relationship[\"target_ref\"]}\"}})\n",
    "            MERGE (sourceObject)-[r:{relationship_name}]->(targetObject)\n",
    "            SET r = $properties\n",
    "        \"\"\"\n",
    "        session.run(query, properties=relationship_properties)\n",
    "\n",
    "\n",
    "#main function to load embedded relationships\n",
    "def load_embedded_relationships(path):\n",
    "    with open(path) as f:\n",
    "        stix_json_data = json.load(f)\n",
    "\n",
    "    ###Matrices to Tactics###\n",
    "\n",
    "    matrix_objects = [obj for obj in stix_json_data[\"objects\"] if obj[\"type\"] == \"x-mitre-matrix\"]\n",
    "\n",
    "    for matrix_obj in matrix_objects:\n",
    "\n",
    "        for tactic_ref_id in matrix_obj[\"tactic_refs\"]:\n",
    "\n",
    "            relationship_type = \"ReferencesTactic\"\n",
    "\n",
    "            relationship_properties = {\n",
    "                \"relationship_type\": relationship_type,\n",
    "                \"source_ref\": matrix_obj[\"id\"],\n",
    "                \"target_ref\": tactic_ref_id\n",
    "            }\n",
    "\n",
    "            query = f\"\"\"\n",
    "                MATCH (sourceObject {{id: \"{matrix_obj[\"id\"]}\"}}), (targetObject {{id: \"{tactic_ref_id}\"}})\n",
    "                MERGE (sourceObject)-[r:{relationship_type}]->(targetObject)\n",
    "                SET r = $properties\n",
    "            \"\"\"\n",
    "            session.run(query, properties=relationship_properties)\n",
    "\n",
    "    ###Tactics to Techniques###\n",
    "\n",
    "    tactic_shortname_to_id = {}\n",
    "    for obj in stix_json_data[\"objects\"]:\n",
    "        if obj[\"type\"] == \"x-mitre-tactic\" and \"x_mitre_shortname\" in obj:\n",
    "            tactic_shortname_to_id[obj[\"x_mitre_shortname\"]] = obj[\"id\"]\n",
    "\n",
    "    attack_patterns = [obj for obj in stix_json_data[\"objects\"] if obj[\"type\"] == \"attack-pattern\"]\n",
    "\n",
    "    for attack_pattern in attack_patterns:\n",
    "        attack_pattern_id = attack_pattern[\"id\"]\n",
    "\n",
    "        if attack_pattern.get(\"kill_chain_phases\"):\n",
    "            for phase in attack_pattern[\"kill_chain_phases\"]:\n",
    "                phase_name = phase[\"phase_name\"]\n",
    "\n",
    "                if phase_name in tactic_shortname_to_id:\n",
    "                    tactic_id = tactic_shortname_to_id[phase_name]\n",
    "\n",
    "                    relationship_type = \"ContainsTechnique\"\n",
    "\n",
    "                    relationship_properties = {\n",
    "                        \"relationship_type\": relationship_type,\n",
    "                        \"source_ref\": tactic_id,\n",
    "                        \"target_ref\": attack_pattern_id,\n",
    "                        \"kill_chain_name\": phase.get(\"kill_chain_name\")\n",
    "                    }\n",
    "\n",
    "                    query = f\"\"\"\n",
    "                            MATCH (sourceObject {{id: \"{tactic_id}\"}}), (targetObject {{id: \"{attack_pattern_id}\"}})\n",
    "                            MERGE (sourceObject)-[r:{relationship_type}]->(targetObject)\n",
    "                            SET r = $properties\n",
    "                        \"\"\"\n",
    "                    session.run(query, properties=relationship_properties)\n",
    "\n",
    "\n",
    "def to_pascal_case(input_string):\n",
    "  words = input_string.split('-')\n",
    "  pascal_case_string = \"\".join(word.capitalize() for word in words)\n",
    "\n",
    "  return pascal_case_string\n",
    "\n",
    "\n",
    "def get_stix_properties_dict(stix_dict):\n",
    "\n",
    "    properties = {}\n",
    "    for attr, value in stix_dict.items():\n",
    "        if isinstance(value, (dict, list)):\n",
    "            properties[attr] = json.dumps(value)\n",
    "        else:\n",
    "            properties[attr] = value\n",
    "\n",
    "    return properties\n",
    "\n",
    "\n",
    "def load_stix_to_neo4j(path: str):\n",
    "    #results = validate_file(path)\n",
    "    #print_results(results)\n",
    "    load_sdos(path)\n",
    "    load_sros(path)\n",
    "    load_embedded_relationships(path)\n",
    "\n",
    "\n",
    "with (driver.session(database=db_name) as session):\n",
    "    load_stix_to_neo4j(\"../attack-stix-data/ics-attack-17.1.json\")\n",
    "    load_stix_to_neo4j(\"../attack-stix-data/mobile-attack-17.1.json\")\n",
    "    load_stix_to_neo4j(\"../attack-stix-data/enterprise-attack-17.1.json\")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "c20ed765-fa41-449b-83c7-9e6cf7fd6aaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:29:39.326120Z",
     "start_time": "2025-06-28T20:29:39.307758Z"
    }
   },
   "source": [
    "create_vector_index(\n",
    "    driver,\n",
    "    name=\"nodes\",\n",
    "    label=\"SDO\",\n",
    "    embedding_property=\"embedding\",\n",
    "    dimensions=768,\n",
    "    similarity_fn=\"cosine\",\n",
    "    neo4j_database=db_name\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "0b5669b1-afe3-401d-ba88-e2a8668b320c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T11:57:44.591614Z",
     "start_time": "2025-06-29T11:57:36.514486Z"
    }
   },
   "source": [
    "with (driver.session(database=db_name) as session):\n",
    "    \n",
    "    result = session.run(\"\"\"\n",
    "    MATCH (n:SDO)\n",
    "    WHERE n.name IS NOT NULL AND n.description IS NOT NULL\n",
    "    OPTIONAL MATCH (n)-[r]->(m)\n",
    "    RETURN n, collect({type: type(r), target: m.name}) AS relationships\n",
    "    \"\"\")\n",
    "\n",
    "    for record in result:\n",
    "        node = record[\"n\"]\n",
    "        relationships = record[\"relationships\"]\n",
    "\n",
    "        base_text = f\"{node['name']}. {node['description']}\"\n",
    "\n",
    "        if relationships:\n",
    "            rel_text = \". \".join(\n",
    "                [f\"Related to {rel['target']} via {rel['type']}\" for rel in relationships if rel[\"target\"]]\n",
    "            )\n",
    "            full_text = f\"{base_text}. {rel_text}\"\n",
    "        else:\n",
    "            full_text = base_text\n",
    "\n",
    "        vector = embedder.embed_query(full_text)\n",
    "\n",
    "        upsert_vectors(\n",
    "            driver,\n",
    "            ids=[node.element_id],\n",
    "            embedding_property=\"embedding\",\n",
    "            embeddings=[vector],\n",
    "            entity_type=EntityType.NODE,\n",
    "            neo4j_database=db_name\n",
    "        )"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 24\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     22\u001B[0m     full_text \u001B[38;5;241m=\u001B[39m base_text\n\u001B[1;32m---> 24\u001B[0m vector \u001B[38;5;241m=\u001B[39m \u001B[43membedder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfull_text\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     26\u001B[0m upsert_vectors(\n\u001B[0;32m     27\u001B[0m     driver,\n\u001B[0;32m     28\u001B[0m     ids\u001B[38;5;241m=\u001B[39m[node\u001B[38;5;241m.\u001B[39melement_id],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     32\u001B[0m     neo4j_database\u001B[38;5;241m=\u001B[39mdb_name\n\u001B[0;32m     33\u001B[0m )\n",
      "File \u001B[1;32m~\\PycharmProjects\\STIX2.1_AI_Project\\.venv\\Lib\\site-packages\\neo4j_graphrag\\embeddings\\ollama.py:52\u001B[0m, in \u001B[0;36mOllamaEmbeddings.embed_query\u001B[1;34m(self, text, **kwargs)\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21membed_query\u001B[39m(\u001B[38;5;28mself\u001B[39m, text: \u001B[38;5;28mstr\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mfloat\u001B[39m]:\n\u001B[0;32m     45\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;124;03m    Generate embeddings for a given query using an Ollama text embedding model.\u001B[39;00m\n\u001B[0;32m     47\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;124;03m        **kwargs (Any): Additional keyword arguments to pass to the Ollama client.\u001B[39;00m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 52\u001B[0m     embeddings_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     53\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     54\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m embeddings_response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m embeddings_response\u001B[38;5;241m.\u001B[39membeddings:\n\u001B[0;32m     59\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m EmbeddingsGenerationError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to retrieve embeddings.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\STIX2.1_AI_Project\\.venv\\Lib\\site-packages\\ollama\\_client.py:367\u001B[0m, in \u001B[0;36mClient.embed\u001B[1;34m(self, model, input, truncate, options, keep_alive)\u001B[0m\n\u001B[0;32m    359\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21membed\u001B[39m(\n\u001B[0;32m    360\u001B[0m   \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    361\u001B[0m   model: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    365\u001B[0m   keep_alive: Optional[Union[\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mstr\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    366\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m EmbedResponse:\n\u001B[1;32m--> 367\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    368\u001B[0m \u001B[43m    \u001B[49m\u001B[43mEmbedResponse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    369\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mPOST\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    370\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m/api/embed\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    371\u001B[0m \u001B[43m    \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mEmbedRequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    372\u001B[0m \u001B[43m      \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    373\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    374\u001B[0m \u001B[43m      \u001B[49m\u001B[43mtruncate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    375\u001B[0m \u001B[43m      \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    376\u001B[0m \u001B[43m      \u001B[49m\u001B[43mkeep_alive\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_alive\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    377\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_dump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexclude_none\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    378\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\STIX2.1_AI_Project\\.venv\\Lib\\site-packages\\ollama\\_client.py:180\u001B[0m, in \u001B[0;36mClient._request\u001B[1;34m(self, cls, stream, *args, **kwargs)\u001B[0m\n\u001B[0;32m    176\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpart)\n\u001B[0;32m    178\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m inner()\n\u001B[1;32m--> 180\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request_raw\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mjson())\n",
      "File \u001B[1;32m~\\PycharmProjects\\STIX2.1_AI_Project\\.venv\\Lib\\site-packages\\ollama\\_client.py:120\u001B[0m, in \u001B[0;36mClient._request_raw\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_request_raw\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    119\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 120\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    121\u001B[0m     r\u001B[38;5;241m.\u001B[39mraise_for_status()\n\u001B[0;32m    122\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m r\n",
      "File \u001B[1;32m~\\PycharmProjects\\STIX2.1_AI_Project\\.venv\\Lib\\site-packages\\httpx\\_client.py:825\u001B[0m, in \u001B[0;36mClient.request\u001B[1;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001B[0m\n\u001B[0;32m    810\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(message, \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m    812\u001B[0m request \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuild_request(\n\u001B[0;32m    813\u001B[0m     method\u001B[38;5;241m=\u001B[39mmethod,\n\u001B[0;32m    814\u001B[0m     url\u001B[38;5;241m=\u001B[39murl,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    823\u001B[0m     extensions\u001B[38;5;241m=\u001B[39mextensions,\n\u001B[0;32m    824\u001B[0m )\n\u001B[1;32m--> 825\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\STIX2.1_AI_Project\\.venv\\Lib\\site-packages\\httpx\\_client.py:914\u001B[0m, in \u001B[0;36mClient.send\u001B[1;34m(self, request, stream, auth, follow_redirects)\u001B[0m\n\u001B[0;32m    910\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_timeout(request)\n\u001B[0;32m    912\u001B[0m auth \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_request_auth(request, auth)\n\u001B[1;32m--> 914\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_handling_auth\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    915\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    916\u001B[0m \u001B[43m    \u001B[49m\u001B[43mauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    917\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    918\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhistory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    919\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    920\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    921\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n",
      "File \u001B[1;32m~\\PycharmProjects\\STIX2.1_AI_Project\\.venv\\Lib\\site-packages\\httpx\\_client.py:942\u001B[0m, in \u001B[0;36mClient._send_handling_auth\u001B[1;34m(self, request, auth, follow_redirects, history)\u001B[0m\n\u001B[0;32m    939\u001B[0m request \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(auth_flow)\n\u001B[0;32m    941\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 942\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_handling_redirects\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    943\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    944\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    945\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhistory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhistory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    946\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    947\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    948\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\STIX2.1_AI_Project\\.venv\\Lib\\site-packages\\httpx\\_client.py:979\u001B[0m, in \u001B[0;36mClient._send_handling_redirects\u001B[1;34m(self, request, follow_redirects, history)\u001B[0m\n\u001B[0;32m    976\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_event_hooks[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequest\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m    977\u001B[0m     hook(request)\n\u001B[1;32m--> 979\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_single_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    980\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    981\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_event_hooks[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
      "File \u001B[1;32m~\\PycharmProjects\\STIX2.1_AI_Project\\.venv\\Lib\\site-packages\\httpx\\_client.py:1014\u001B[0m, in \u001B[0;36mClient._send_single_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m   1009\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m   1010\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAttempted to send an async request with a sync Client instance.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1011\u001B[0m     )\n\u001B[0;32m   1013\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request\u001B[38;5;241m=\u001B[39mrequest):\n\u001B[1;32m-> 1014\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mtransport\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1016\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response\u001B[38;5;241m.\u001B[39mstream, SyncByteStream)\n\u001B[0;32m   1018\u001B[0m response\u001B[38;5;241m.\u001B[39mrequest \u001B[38;5;241m=\u001B[39m request\n",
      "File \u001B[1;32m~\\PycharmProjects\\STIX2.1_AI_Project\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001B[0m, in \u001B[0;36mHTTPTransport.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    237\u001B[0m req \u001B[38;5;241m=\u001B[39m httpcore\u001B[38;5;241m.\u001B[39mRequest(\n\u001B[0;32m    238\u001B[0m     method\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mmethod,\n\u001B[0;32m    239\u001B[0m     url\u001B[38;5;241m=\u001B[39mhttpcore\u001B[38;5;241m.\u001B[39mURL(\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    247\u001B[0m     extensions\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mextensions,\n\u001B[0;32m    248\u001B[0m )\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[1;32m--> 250\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_pool\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    252\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(resp\u001B[38;5;241m.\u001B[39mstream, typing\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[0;32m    254\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Response(\n\u001B[0;32m    255\u001B[0m     status_code\u001B[38;5;241m=\u001B[39mresp\u001B[38;5;241m.\u001B[39mstatus,\n\u001B[0;32m    256\u001B[0m     headers\u001B[38;5;241m=\u001B[39mresp\u001B[38;5;241m.\u001B[39mheaders,\n\u001B[0;32m    257\u001B[0m     stream\u001B[38;5;241m=\u001B[39mResponseStream(resp\u001B[38;5;241m.\u001B[39mstream),\n\u001B[0;32m    258\u001B[0m     extensions\u001B[38;5;241m=\u001B[39mresp\u001B[38;5;241m.\u001B[39mextensions,\n\u001B[0;32m    259\u001B[0m )\n",
      "File \u001B[1;32m~\\PycharmProjects\\STIX2.1_AI_Project\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001B[0m, in \u001B[0;36mConnectionPool.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    253\u001B[0m         closing \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_assign_requests_to_connections()\n\u001B[0;32m    255\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_connections(closing)\n\u001B[1;32m--> 256\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001B[39;00m\n\u001B[0;32m    259\u001B[0m \u001B[38;5;66;03m# the point at which the response is closed.\u001B[39;00m\n\u001B[0;32m    260\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response\u001B[38;5;241m.\u001B[39mstream, typing\u001B[38;5;241m.\u001B[39mIterable)\n",
      "File \u001B[1;32m~\\PycharmProjects\\STIX2.1_AI_Project\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001B[0m, in \u001B[0;36mConnectionPool.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    232\u001B[0m connection \u001B[38;5;241m=\u001B[39m pool_request\u001B[38;5;241m.\u001B[39mwait_for_connection(timeout\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[0;32m    234\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    235\u001B[0m     \u001B[38;5;66;03m# Send the request on the assigned connection.\u001B[39;00m\n\u001B[1;32m--> 236\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    237\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpool_request\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\n\u001B[0;32m    238\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    239\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ConnectionNotAvailable:\n\u001B[0;32m    240\u001B[0m     \u001B[38;5;66;03m# In some cases a connection may initially be available to\u001B[39;00m\n\u001B[0;32m    241\u001B[0m     \u001B[38;5;66;03m# handle a request, but then become unavailable.\u001B[39;00m\n\u001B[0;32m    242\u001B[0m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m    243\u001B[0m     \u001B[38;5;66;03m# In this case we clear the connection and try again.\u001B[39;00m\n\u001B[0;32m    244\u001B[0m     pool_request\u001B[38;5;241m.\u001B[39mclear_connection()\n",
      "File \u001B[1;32m~\\PycharmProjects\\STIX2.1_AI_Project\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001B[0m, in \u001B[0;36mHTTPConnection.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    100\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connect_failed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    101\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[1;32m--> 103\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_connection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\STIX2.1_AI_Project\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001B[0m, in \u001B[0;36mHTTP11Connection.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    134\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse_closed\u001B[39m\u001B[38;5;124m\"\u001B[39m, logger, request) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[0;32m    135\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_response_closed()\n\u001B[1;32m--> 136\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "File \u001B[1;32m~\\PycharmProjects\\STIX2.1_AI_Project\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001B[0m, in \u001B[0;36mHTTP11Connection.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m     95\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m     97\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\n\u001B[0;32m     98\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreceive_response_headers\u001B[39m\u001B[38;5;124m\"\u001B[39m, logger, request, kwargs\n\u001B[0;32m     99\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[0;32m    100\u001B[0m     (\n\u001B[0;32m    101\u001B[0m         http_version,\n\u001B[0;32m    102\u001B[0m         status,\n\u001B[0;32m    103\u001B[0m         reason_phrase,\n\u001B[0;32m    104\u001B[0m         headers,\n\u001B[0;32m    105\u001B[0m         trailing_data,\n\u001B[1;32m--> 106\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_receive_response_headers\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    107\u001B[0m     trace\u001B[38;5;241m.\u001B[39mreturn_value \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    108\u001B[0m         http_version,\n\u001B[0;32m    109\u001B[0m         status,\n\u001B[0;32m    110\u001B[0m         reason_phrase,\n\u001B[0;32m    111\u001B[0m         headers,\n\u001B[0;32m    112\u001B[0m     )\n\u001B[0;32m    114\u001B[0m network_stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_network_stream\n",
      "File \u001B[1;32m~\\PycharmProjects\\STIX2.1_AI_Project\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001B[0m, in \u001B[0;36mHTTP11Connection._receive_response_headers\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    174\u001B[0m timeout \u001B[38;5;241m=\u001B[39m timeouts\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mread\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    176\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 177\u001B[0m     event \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_receive_event\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    178\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(event, h11\u001B[38;5;241m.\u001B[39mResponse):\n\u001B[0;32m    179\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\STIX2.1_AI_Project\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001B[0m, in \u001B[0;36mHTTP11Connection._receive_event\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    214\u001B[0m     event \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_h11_state\u001B[38;5;241m.\u001B[39mnext_event()\n\u001B[0;32m    216\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m event \u001B[38;5;129;01mis\u001B[39;00m h11\u001B[38;5;241m.\u001B[39mNEED_DATA:\n\u001B[1;32m--> 217\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_network_stream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    218\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mREAD_NUM_BYTES\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[0;32m    219\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    221\u001B[0m     \u001B[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001B[39;00m\n\u001B[0;32m    222\u001B[0m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m    223\u001B[0m     \u001B[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    227\u001B[0m     \u001B[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001B[39;00m\n\u001B[0;32m    228\u001B[0m     \u001B[38;5;66;03m# it as a ConnectError.\u001B[39;00m\n\u001B[0;32m    229\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;241m==\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_h11_state\u001B[38;5;241m.\u001B[39mtheir_state \u001B[38;5;241m==\u001B[39m h11\u001B[38;5;241m.\u001B[39mSEND_RESPONSE:\n",
      "File \u001B[1;32m~\\PycharmProjects\\STIX2.1_AI_Project\\.venv\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001B[0m, in \u001B[0;36mSyncStream.read\u001B[1;34m(self, max_bytes, timeout)\u001B[0m\n\u001B[0;32m    126\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[0;32m    127\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39msettimeout(timeout)\n\u001B[1;32m--> 128\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmax_bytes\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "ad125991-b142-4b39-bbed-d3b7a0a7aedd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T13:13:41.812078Z",
     "start_time": "2025-06-29T13:13:41.805554Z"
    }
   },
   "source": [
    "def get_neighborhood(driver, node_id):\n",
    "    with (driver.session(database=db_name) as session):\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (n)-[r]-(m)\n",
    "            WHERE n.id = $id\n",
    "            RETURN DISTINCT m, type(r) AS rel_type, r.description AS rel_desc\n",
    "        \"\"\", id=node_id)\n",
    "        return [(record[\"m\"], record[\"rel_type\"], record[\"rel_desc\"]) for record in result]\n",
    "\n",
    "def build_question_context(main_node, neighbors):\n",
    "    parts = []\n",
    "\n",
    "    parts.append(\"Best similarity search (the main node) is: \" + f\"\"\"\"{main_node.get('name')}\" of type \"{main_node.get('type')}\". Description of \"{main_node.get('name')}\": {main_node.get('description')}\"\"\")\n",
    "    parts.append(\"\\nThe main node's neighbors are the following nodes:\")\n",
    "    \n",
    "    for neighbor, rel_type, rel_desc in neighbors:\n",
    "        parts.append(f\"\"\"\n",
    "+++++ {neighbor.get('name').upper()} +++++\\n\n",
    "Node \"{neighbor.get('name')}\" of type \"{neighbor.get('type')}\". Description of \"{neighbor.get('name')}\": {neighbor.get('description')}\\n\n",
    "The main node is related to \"{neighbor.get('name')}\" via type \"{rel_type}\". This relationship contains the following description: {rel_desc}\"\"\")\n",
    "\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "def approach2(query_text):\n",
    "    #query_text = \"What are the names of 2 attack patterns used by WannaCry malware?\"\n",
    "    \n",
    "    retriever = VectorRetriever(driver, \"nodes\", embedder, neo4j_database=db_name)\n",
    "    result = retriever.search(query_text=query_text, top_k=1) #similarity search to get the closest match based on the query_text\n",
    "    \n",
    "    if result.items == []:\n",
    "        raise ValueError(\"Expected items, but got None\")\n",
    "\n",
    "    question_context = \"\"\n",
    "    for item in result.items:\n",
    "        dict_item = ast.literal_eval(item.content)\n",
    "    \n",
    "        #getting neighbours of the closest match node and saving their info to question_context\n",
    "        neighbors_of_main_item = get_neighborhood(driver, dict_item[\"id\"])\n",
    "        question_context += build_question_context(dict_item, neighbors_of_main_item)\n",
    "    \n",
    "    print(f\"\"\"DEBUG question_context:\\n{question_context}\\n{\"#\" * 50}\"\"\")\n",
    "    print(f\"\"\"Question: {query_text}\\n\"\"\")\n",
    "    \n",
    "    # asking llm the question, but now with question_context from the graph\n",
    "    response = llm.invoke(\n",
    "        input=query_text,\n",
    "        system_instruction=question_context\n",
    "    )\n",
    "    print(f\"\"\"Response:\\n{response.content}\"\"\")\n",
    "    return response.content.split(\"</think>\")[1]\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "057b7dcc-c677-4d50-b160-0b8a65a3fc5c",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "id": "ffbabb65-d3eb-4c44-987a-e3e01de7f24f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "with open(\n",
    "        \"../AttackSeq-Technique-Test.csv\", mode=\"r\", newline=\"\", encoding=\"utf-8\"\n",
    ") as infile, open(\"../approach2.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as outfile:\n",
    "\n",
    "    reader = csv.DictReader(infile)\n",
    "    fieldnames = [\"Question ID\", \"Answer\", \"Latency\"]\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in reader:\n",
    "        question_id = row.get(\"Question ID\", \"\")\n",
    "        question_text = row.get(\"Question\", \"\")\n",
    "\n",
    "        print(f\"---------- {question_id} ----------\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        answer = approach2(question_text)\n",
    "        latency = round(time.time() - start_time, 4)\n",
    "\n",
    "        writer.writerow({\n",
    "            \"Question ID\": question_id,\n",
    "            \"Answer\": answer,\n",
    "            \"Latency\": latency\n",
    "        })\n",
    "        \n",
    "driver.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import csv\n",
    "import time\n",
    "import ast\n",
    "# from neo4j import GraphDatabase\n",
    "# from neo4j_graphrag.embeddings import OllamaEmbeddings\n",
    "# from neo4j_graphrag.retrievers import VectorRetriever\n",
    "# from neo4j_graphrag.llm import OllamaLLM\n",
    "\n",
    "# driver = GraphDatabase.driver(...)\n",
    "# embedder = OllamaEmbeddings(...)\n",
    "# llm = OllamaLLM(...)\n",
    "# db_name = \"...\"\n",
    "\n",
    "def get_neighborhood(driver, node_id):\n",
    "\n",
    "    with (driver.session(database=db_name) as session):\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (n)-[r]-(m)\n",
    "            WHERE n.id = $id\n",
    "            RETURN DISTINCT m, type(r) AS rel_type, r.description AS rel_desc\n",
    "        \"\"\", id=node_id)\n",
    "        return [(record[\"m\"], record[\"rel_type\"], record[\"rel_desc\"]) for record in result]\n",
    "\n",
    "def build_question_context(main_node, neighbors):\n",
    "    parts = []\n",
    "    parts.append(\"Best similarity search (the main node) is: \" + f\"\"\"\"{main_node.get('name')}\" of type \"{main_node.get('type')}\". Description of \"{main_node.get('name')}\": {main_node.get('description')}\"\"\")\n",
    "    parts.append(\"\\nThe main node's neighbors are the following nodes:\")\n",
    "\n",
    "    for neighbor, rel_type, rel_desc in neighbors:\n",
    "        parts.append(f\"\"\"\n",
    "+++++ {neighbor.get('name').upper()} +++++\n",
    "Node \"{neighbor.get('name')}\" of type \"{neighbor.get('type')}\". Description of \"{neighbor.get('name')}\": {neighbor.get('description')}\n",
    "The main node is related to \"{neighbor.get('name')}\" via type \"{rel_type}\". This relationship contains the following description: {rel_desc}\"\"\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "# Main approach function\n",
    "def evaluate_approach(query_text, add_rag_context=False, add_choices=False, choices=None):\n",
    "\n",
    "    question_context = \"\"\n",
    "    if add_rag_context:\n",
    "        # Initialize VectorRetriever\n",
    "        retriever = VectorRetriever(driver, \"nodes\", embedder, neo4j_database=db_name)\n",
    "        result = retriever.search(query_text=query_text, top_k=1)\n",
    "\n",
    "        if result.items:\n",
    "            for item in result.items:\n",
    "                dict_item = ast.literal_eval(item.content)\n",
    "                neighbors_of_main_item = get_neighborhood(driver, dict_item[\"id\"])\n",
    "                question_context += build_question_context(dict_item, neighbors_of_main_item)\n",
    "        else:\n",
    "            print(f\"Warning: No relevant nodes found for query: '{query_text}'\")\n",
    "\n",
    "    full_query = query_text\n",
    "    if add_choices and choices:\n",
    "        choices_str = \", \".join(choices)\n",
    "        full_query += f\"\\n\\nChoose from the following options: {choices_str}\"\n",
    "\n",
    "    system_instruction = \"\"\n",
    "    if add_rag_context:\n",
    "        system_instruction = question_context\n",
    "        # Instruct the LLM to use the provided context\n",
    "        system_instruction += \"\\n\\nBased on the provided context and the question, please provide the most accurate answer. Start your answer after </think>.\"\n",
    "    else:\n",
    "        system_instruction = \"Please answer the following question. Start your answer after </think>.\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = llm.invoke(\n",
    "        input=full_query,\n",
    "        system_instruction=system_instruction\n",
    "    )\n",
    "    latency = round(time.time() - start_time, 4)\n",
    "\n",
    "    full_response_content = response.content\n",
    "\n",
    "    thinking_part = \"\"\n",
    "    answer_part = full_response_content\n",
    "    if \"</think>\" in full_response_content:\n",
    "        parts = full_response_content.split(\"</think>\", 1)\n",
    "        thinking_part = parts[0].strip()\n",
    "        answer_part = parts[1].strip()\n",
    "\n",
    "    len_thinking = len(thinking_part)\n",
    "    len_answer = len(answer_part)\n",
    "\n",
    "    return answer_part, latency, len_thinking, len_answer\n",
    "\n",
    "# --- Evaluation Script ---\n",
    "\n",
    "# output CSV filenames\n",
    "input_csv_filename = \"../AttackSeq-Technique-Test.csv\"\n",
    "output_csv_filename = \"evaluation_results.csv\"\n",
    "\n",
    "# headers for the output CSV file\n",
    "fieldnames = [\n",
    "    \"Question ID\",\n",
    "    \"Question\",\n",
    "    \"Ground Truth\",\n",
    "    \"Answer LLM (Question Only)\", \"Duration (QO)\", \"Length Thinking (QO)\", \"Length Answer (QO)\", \"Correctness (QO)\",\n",
    "    \"Answer LLM (RAG)\", \"Duration (RAG)\", \"Length Thinking (RAG)\", \"Length Answer (RAG)\", \"Correctness (RAG)\",\n",
    "    \"Answer LLM (Choices)\", \"Duration (Choices)\", \"Length Thinking (Choices)\", \"Length Answer (Choices)\", \"Correctness (Choices)\"\n",
    "]\n",
    "\n",
    "print(f\"Starting evaluation from '{input_csv_filename}'...\")\n",
    "with open(input_csv_filename, mode=\"r\", newline=\"\", encoding=\"utf-8\") as infile, \\\n",
    "     open(output_csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as outfile:\n",
    "\n",
    "    reader = csv.DictReader(infile)\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in reader:\n",
    "        question_id = row.get(\"Question ID\", \"\")\n",
    "        question_text = row.get(\"Question\", \"\")\n",
    "        ground_truth = row.get(\"Ground Truth\", \"\").strip().lower()\n",
    "\n",
    "        unshuffled_choices_str = row.get(\"Unshuffled Choices\", \"\")\n",
    "        unshuffled_choices = [c.strip() for c in unshuffled_choices_str.split(',')] if unshuffled_choices_str else []\n",
    "\n",
    "        print(f\"\\n---------- {question_id} ----------\")\n",
    "        print(f\"Question: {question_text}\")\n",
    "\n",
    "        results = {\n",
    "            \"Question ID\": question_id,\n",
    "            \"Question\": question_text,\n",
    "            \"Ground Truth\": row.get(\"Ground Truth\", \"\") # Store original ground truth\n",
    "        }\n",
    "\n",
    "        # --- Scenario 1: Question Only ---\n",
    "        answer_qo, latency_qo, len_thinking_qo, len_answer_qo = evaluate_approach(question_text, add_rag_context=False, add_choices=False)\n",
    "        correctness_qo = \"Correct\" if answer_qo.strip().lower() == ground_truth else \"Incorrect\"\n",
    "        results.update({\n",
    "            \"Answer LLM (Question Only)\": answer_qo,\n",
    "            \"Duration (QO)\": latency_qo,\n",
    "            \"Length Thinking (QO)\": len_thinking_qo,\n",
    "            \"Length Answer (QO)\": len_answer_qo,\n",
    "            \"Correctness (QO)\": correctness_qo\n",
    "        })\n",
    "        print(f\"  [QO] Answer: '{answer_qo}' | Correct: {correctness_qo}\")\n",
    "\n",
    "        # --- Scenario 2: Question + RAG Graph Knowledge ---\n",
    "        answer_rag, latency_rag, len_thinking_rag, len_answer_rag = evaluate_approach(question_text, add_rag_context=True, add_choices=False)\n",
    "        correctness_rag = \"Correct\" if answer_rag.strip().lower() == ground_truth else \"Incorrect\"\n",
    "        results.update({\n",
    "            \"Answer LLM (RAG)\": answer_rag,\n",
    "            \"Duration (RAG)\": latency_rag,\n",
    "            \"Length Thinking (RAG)\": len_thinking_rag,\n",
    "            \"Length Answer (RAG)\": len_answer_rag,\n",
    "            \"Correctness (RAG)\": correctness_rag\n",
    "        })\n",
    "        print(f\"  [RAG] Answer: '{answer_rag}' | Correct: {correctness_rag}\")\n",
    "\n",
    "        # --- Scenario 3: Question + RAG Graph Knowledge + Answer Choices ---\n",
    "        answer_choices, latency_choices, len_thinking_choices, len_answer_choices = evaluate_approach(question_text, add_rag_context=True, add_choices=True, choices=unshuffled_choices)\n",
    "        correctness_choices = \"Correct\" if answer_choices.strip().lower() == ground_truth else \"Incorrect\"\n",
    "        results.update({\n",
    "            \"Answer LLM (Choices)\": answer_choices,\n",
    "            \"Duration (Choices)\": latency_choices,\n",
    "            \"Length Thinking (Choices)\": len_thinking_choices,\n",
    "            \"Length Answer (Choices)\": len_answer_choices,\n",
    "            \"Correctness (Choices)\": correctness_choices\n",
    "        })\n",
    "        print(f\"  [Choices] Answer: '{answer_choices}' | Correct: {correctness_choices}\")\n",
    "\n",
    "        writer.writerow(results)\n",
    "\n",
    "driver.close()\n",
    "print(f\"\\nEvaluation complete. Results saved to '{output_csv_filename}'\")"
   ],
   "id": "8a4b2c4581cc3835"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import csv\n",
    "import time\n",
    "import ast\n",
    "\n",
    "# Annahme: Imports und Initialisierungen bereits vorhanden\n",
    "# from neo4j import GraphDatabase\n",
    "# from neo4j_graphrag.embeddings import OllamaEmbeddings\n",
    "# from neo4j_graphrag.retrievers import VectorRetriever\n",
    "# from neo4j_graphrag.llm import OllamaLLM\n",
    "\n",
    "# Annahme: 'driver', 'embedder', 'llm', 'db_name'  global verfügbar\n",
    "# from os import getenv\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv(\".env\")\n",
    "# db_uri = getenv(\"db_uri\")\n",
    "# db_name = getenv(\"db_name\")\n",
    "# db_username = getenv(\"db_username\")\n",
    "# db_password = getenv(\"db_password\")\n",
    "# auth = (db_username, db_password)\n",
    "# driver = GraphDatabase.driver(uri=db_uri, auth=auth)\n",
    "# embedder = OllamaEmbeddings(model=\"nomic-embed-text\") # Oder Ihr spezifisches Modell\n",
    "# llm = OllamaLLM(model_name=\"deepseek-r1:1.5b\") # Oder Ihr spezifisches Modell\n",
    "\n",
    "\n",
    "def get_neighborhood(driver, node_id):\n",
    "\n",
    "    with (driver.session(database=db_name) as session):\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (n)-[r]-(m)\n",
    "            WHERE n.id = $id\n",
    "            RETURN DISTINCT m, type(r) AS rel_type, r.description AS rel_desc\n",
    "        \"\"\", id=node_id)\n",
    "        return [(record[\"m\"], record[\"rel_type\"], record[\"rel_desc\"]) for record in result]\n",
    "\n",
    "def build_question_context(main_node, neighbors):\n",
    "\n",
    "    parts = []\n",
    "    parts.append(\"Best similarity search (the main node) is: \" + f\"\"\"\"{main_node.get('name')}\" of type \"{main_node.get('type')}\". Description of \"{main_node.get('name')}\": {main_node.get('description')}\"\"\")\n",
    "    parts.append(\"\\nThe main node's neighbors are the following nodes:\")\n",
    "\n",
    "    for neighbor, rel_type, rel_desc in neighbors:\n",
    "        parts.append(f\"\"\"\n",
    "+++++ {neighbor.get('name').upper()} +++++\n",
    "Node \"{neighbor.get('name')}\" of type \"{neighbor.get('type')}\". Description of \"{neighbor.get('name')}\": {neighbor.get('description')}\n",
    "The main node is related to \"{neighbor.get('name')}\" via type \"{rel_type}\". This relationship contains the following description: {rel_desc}\"\"\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "# --- Ende der Funktionen aus Ihrem ursprünglichen Ansatz ---\n",
    "\n",
    "\n",
    "# Haupt-Ansatzfunktion (modifiziert für 3 Szenarien und DEBUG-Ausgabe)\n",
    "def evaluate_approach(query_text, add_rag_context=False, add_choices=False, choices=None):\n",
    "\n",
    "    question_context = \"\"\n",
    "    if add_rag_context:\n",
    "        # Initialisiere VectorRetriever\n",
    "        retriever = VectorRetriever(driver, \"nodes\", embedder, neo4j_database=db_name)\n",
    "        result = retriever.search(query_text=query_text, top_k=1)\n",
    "\n",
    "        if result.items:\n",
    "            for item in result.items:\n",
    "                # item.content wird als String-Repräsentation\n",
    "                dict_item = ast.literal_eval(item.content)\n",
    "                print(f\"DEBUG: Vom Retriever gefundener Knoten (Main Node): {dict_item.get('name')} (ID: {dict_item.get('id')})\")\n",
    "                neighbors_of_main_item = get_neighborhood(driver, dict_item[\"id\"])\n",
    "                question_context += build_question_context(dict_item, neighbors_of_main_item)\n",
    "        else:\n",
    "            print(f\"Warning: Keine relevanten Knoten für die Abfrage gefunden: '{query_text}'\")\n",
    "\n",
    "    full_query = query_text\n",
    "    if add_choices and choices:\n",
    "        choices_str = \", \".join(choices)\n",
    "        full_query += f\"\\n\\nChoose from the following options: {choices_str}\"\n",
    "\n",
    "    system_instruction = \"\"\n",
    "    if add_rag_context:\n",
    "        system_instruction = question_context\n",
    "        # Weist das LLM an\n",
    "        system_instruction += \"\\n\\nBasierend auf dem bereitgestellten Kontext und der Frage, geben Sie bitte die genaueste Antwort. Beginnen Sie Ihre Antwort nach </think>.\"\n",
    "    else:\n",
    "        system_instruction = \"Bitte beantworten Sie die folgende Frage. Beginnen Sie Ihre Antwort nach </think>.\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = llm.invoke(\n",
    "        input=full_query,\n",
    "        system_instruction=system_instruction\n",
    "    )\n",
    "    latency = round(time.time() - start_time, 4)\n",
    "\n",
    "    full_response_content = response.content\n",
    "\n",
    "    # Trenne die Antwort </think>-Token\n",
    "    thinking_part = \"\"\n",
    "    answer_part = full_response_content\n",
    "    if \"</think>\" in full_response_content:\n",
    "        parts = full_response_content.split(\"</think>\", 1)\n",
    "        thinking_part = parts[0].strip()\n",
    "        answer_part = parts[1].strip()\n",
    "\n",
    "    len_thinking = len(thinking_part)\n",
    "    len_answer = len(answer_part)\n",
    "\n",
    "    return answer_part, latency, len_thinking, len_answer\n",
    "\n",
    "# --- Evaluationsskript ---\n",
    "\n",
    "# Definiere die Namen der Input- und Output-CSV-Dateien\n",
    "input_csv_filename = \"../AttackSeq-Technique-Test.csv\"\n",
    "output_csv_filename = \"evaluation_results.csv\"\n",
    "\n",
    "# Definiere die Header für die Output-CSV-Datei\n",
    "fieldnames = [\n",
    "    \"Question ID\",\n",
    "    \"Question\",\n",
    "    \"Ground Truth\",\n",
    "    \"Answer LLM (Question Only)\", \"Duration (QO)\", \"Length Thinking (QO)\", \"Length Answer (QO)\", \"Correctness (QO)\",\n",
    "    \"Answer LLM (RAG)\", \"Duration (RAG)\", \"Length Thinking (RAG)\", \"Length Answer (RAG)\", \"Correctness (RAG)\",\n",
    "    \"Answer LLM (Choices)\", \"Duration (Choices)\", \"Length Thinking (Choices)\", \"Length Answer (Choices)\", \"Correctness (Choices)\"\n",
    "]\n",
    "\n",
    "print(f\"Starte die Evaluation von '{input_csv_filename}'...\")\n",
    "with open(input_csv_filename, mode=\"r\", newline=\"\", encoding=\"utf-8\") as infile, \\\n",
    "     open(output_csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as outfile:\n",
    "\n",
    "    reader = csv.DictReader(infile)\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in reader:\n",
    "        question_id = row.get(\"Question ID\", \"\")\n",
    "        question_text = row.get(\"Question\", \"\")\n",
    "        ground_truth = row.get(\"Ground Truth\", \"\").strip().lower()\n",
    "\n",
    "        unshuffled_choices_str = row.get(\"Unshuffled Choices\", \"\")\n",
    "        unshuffled_choices = [c.strip() for c in unshuffled_choices_str.split(',')] if unshuffled_choices_str else []\n",
    "\n",
    "        print(f\"\\n---------- {question_id} ----------\")\n",
    "        print(f\"Question: {question_text}\")\n",
    "\n",
    "        results = {\n",
    "            \"Question ID\": question_id,\n",
    "            \"Question\": question_text,\n",
    "            \"Ground Truth\": row.get(\"Ground Truth\", \"\")\n",
    "        }\n",
    "\n",
    "        # --- Szenario 1: Nur Frage ---\n",
    "        answer_qo, latency_qo, len_thinking_qo, len_answer_qo = evaluate_approach(question_text, add_rag_context=False, add_choices=False)\n",
    "        correctness_qo = \"Correct\" if ground_truth in answer_qo.strip().lower() else \"Incorrect\"\n",
    "        results.update({\n",
    "            \"Answer LLM (Question Only)\": answer_qo,\n",
    "            \"Duration (QO)\": latency_qo,\n",
    "            \"Length Thinking (QO)\": len_thinking_qo,\n",
    "            \"Length Answer (QO)\": len_answer_qo,\n",
    "            \"Correctness (QO)\": correctness_qo\n",
    "        })\n",
    "        print(f\"  [QO] Answer: '{answer_qo}' | Correct: {correctness_qo}\")\n",
    "\n",
    "        # --- Szenario 2: Frage + RAG Graph Wissen ---\n",
    "        answer_rag, latency_rag, len_thinking_rag, len_answer_rag = evaluate_approach(question_text, add_rag_context=True, add_choices=False)\n",
    "        correctness_rag = \"Correct\" if ground_truth in answer_rag.strip().lower() else \"Incorrect\"\n",
    "        results.update({\n",
    "            \"Answer LLM (RAG)\": answer_rag,\n",
    "            \"Duration (RAG)\": latency_rag,\n",
    "            \"Length Thinking (RAG)\": len_thinking_rag,\n",
    "            \"Length Answer (RAG)\": len_answer_rag,\n",
    "            \"Correctness (RAG)\": correctness_rag\n",
    "        })\n",
    "        print(f\"  [RAG] Answer: '{answer_rag}' | Correct: {correctness_rag}\")\n",
    "\n",
    "        # --- Szenario 3: Frage + RAG Graph Wissen + Antwortmöglichkeiten ---\n",
    "        answer_choices, latency_choices, len_thinking_choices, len_answer_choices = evaluate_approach(question_text, add_rag_context=True, add_choices=True, choices=unshuffled_choices)\n",
    "        correctness_choices = \"Correct\" if ground_truth in answer_choices.strip().lower() else \"Incorrect\"\n",
    "        results.update({\n",
    "            \"Answer LLM (Choices)\": answer_choices,\n",
    "            \"Duration (Choices)\": latency_choices,\n",
    "            \"Length Thinking (Choices)\": len_thinking_choices,\n",
    "            \"Length Answer (Choices)\": len_answer_choices,\n",
    "            \"Correctness (Choices)\": correctness_choices\n",
    "        })\n",
    "        print(f\"  [Choices] Answer: '{answer_choices}' | Correct: {correctness_choices}\")\n",
    "\n",
    "        writer.writerow(results)\n",
    "\n",
    "driver.close()\n",
    "print(f\"\\nEvaluation abgeschlossen. Ergebnisse gespeichert in '{output_csv_filename}'\")"
   ],
   "id": "a34446a984961300"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import csv\n",
    "import time\n",
    "import ast\n",
    "\n",
    "# from neo4j import GraphDatabase\n",
    "# from neo4j_graphrag.embeddings import OllamaEmbeddings\n",
    "# from neo4j_graphrag.retrievers import VectorRetriever\n",
    "# from neo4j_graphrag.llm import OllamaLLM\n",
    "\n",
    "# Beispiel (falls noch nicht geschehen, ersetzen Sie die Platzhalter):\n",
    "# from os import getenv\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv(\".env\")\n",
    "# db_uri = getenv(\"db_uri\")\n",
    "# db_name = getenv(\"db_name\")\n",
    "# db_username = getenv(\"db_username\")\n",
    "# db_password = getenv(\"db_password\")\n",
    "# auth = (db_username, db_password)\n",
    "# driver = GraphDatabase.driver(uri=db_uri, auth=auth)\n",
    "# embedder = OllamaEmbeddings(model=\"nomic-embed-text\") # Oder Ihr spezifisches Modell\n",
    "# llm = OllamaLLM(model_name=\"deepseek-r1:1.5b\") # Oder Ihr spezifisches Modell\n",
    "\n",
    "def get_neighborhood(driver, node_id):\n",
    "\n",
    "    with (driver.session(database=db_name) as session):\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (n)-[r]-(m)\n",
    "            WHERE n.id = $id\n",
    "            RETURN DISTINCT m, type(r) AS rel_type, r.description AS rel_desc\n",
    "        \"\"\", id=node_id)\n",
    "        return [(record[\"m\"], record[\"rel_type\"], record[\"rel_desc\"]) for record in result]\n",
    "\n",
    "def build_question_context(main_node, neighbors):\n",
    "\n",
    "    parts = []\n",
    "    parts.append(\"Best similarity search (the main node) is: \" + f\"\"\"\"{main_node.get('name')}\" of type \"{main_node.get('type')}\". Description of \"{main_node.get('name')}\": {main_node.get('description')}\"\"\")\n",
    "    parts.append(\"\\nThe main node's neighbors are the following nodes:\")\n",
    "\n",
    "    for neighbor, rel_type, rel_desc in neighbors:\n",
    "        parts.append(f\"\"\"\n",
    "+++++ {neighbor.get('name').upper()} +++++\n",
    "Node \"{neighbor.get('name')}\" of type \"{neighbor.get('type')}\". Description of \"{neighbor.get('name')}\": {neighbor.get('description')}\n",
    "The main node is related to \"{neighbor.get('name')}\" via type \"{rel_type}\". This relationship contains the following description: {rel_desc}\"\"\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "def evaluate_approach(query_text, add_rag_context=False, add_choices=False, choices=None):\n",
    "\n",
    "    question_context = \"\"\n",
    "    found_node_info = \"\"\n",
    "\n",
    "    if add_rag_context:\n",
    "        retriever = VectorRetriever(driver, \"nodes\", embedder, neo4j_database=db_name)\n",
    "        result = retriever.search(query_text=query_text, top_k=1)\n",
    "\n",
    "        if result.items:\n",
    "            for item in result.items:\n",
    "                dict_item = ast.literal_eval(item.content)\n",
    "\n",
    "                found_node_info = f\"DEBUG: Gefundener Knoten (Main Node): Name='{dict_item.get('name')}', Type='{dict_item.get('type')}', ID='{dict_item.get('id')}'\"\n",
    "\n",
    "                neighbors_of_main_item = get_neighborhood(driver, dict_item[\"id\"])\n",
    "                question_context += build_question_context(dict_item, neighbors_of_main_item)\n",
    "        else:\n",
    "            found_node_info = f\"Warning: Keine relevanten Knoten für die Abfrage gefunden: '{query_text}'\"\n",
    "\n",
    "    full_query = query_text\n",
    "    if add_choices and choices:\n",
    "        choices_str = \", \".join(choices)\n",
    "        full_query += f\"\\n\\nChoose from the following options: {choices_str}\"\n",
    "\n",
    "    system_instruction = \"\"\n",
    "    if add_rag_context:\n",
    "        system_instruction = question_context\n",
    "        # Weist das LLM an, den bereitgestellten Kontext zu verwenden\n",
    "        system_instruction += \"\\n\\nAs an IT security expert, based on the provided context and the question, please provide the most accurate answer in English. Start your answer after </think>.\"\n",
    "    else:\n",
    "        # System-Anweisung für den \"Nur Frage\"-Modus\n",
    "        system_instruction = \"You are an IT security expert. Please answer the following question in English. Start your answer after </think>.\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = llm.invoke(\n",
    "        input=full_query,\n",
    "        system_instruction=system_instruction\n",
    "    )\n",
    "    latency = round(time.time() - start_time, 4)\n",
    "\n",
    "    full_response_content = response.content\n",
    "\n",
    "    thinking_part = \"\"\n",
    "    answer_part = full_response_content\n",
    "    if \"</think>\" in full_response_content:\n",
    "        parts = full_response_content.split(\"</think>\", 1)\n",
    "        thinking_part = parts[0].strip()\n",
    "        answer_part = parts[1].strip()\n",
    "\n",
    "    len_thinking = len(thinking_part)\n",
    "    len_answer = len(answer_part)\n",
    "\n",
    "    return answer_part, latency, len_thinking, len_answer, found_node_info\n",
    "\n",
    "\n",
    "# --- Evaluationsskript ---\n",
    "\n",
    "input_csv_filename = \"../AttackSeq-Technique-Test.csv\"\n",
    "output_csv_filename = \"evaluation_results.csv\"\n",
    "\n",
    "fieldnames = [\n",
    "    \"Question ID\",\n",
    "    \"Question\",\n",
    "    \"Ground Truth\",\n",
    "    \"Answer LLM (Question Only)\", \"Duration (QO)\", \"Length Thinking (QO)\", \"Length Answer (QO)\", \"Correctness (QO)\",\n",
    "    \"Answer LLM (RAG)\", \"Duration (RAG)\", \"Length Thinking (RAG)\", \"Length Answer (RAG)\", \"Correctness (RAG)\", \"Found Node RAG\",\n",
    "    \"Answer LLM (Choices)\", \"Duration (Choices)\", \"Length Thinking (Choices)\", \"Length Answer (Choices)\", \"Correctness (Choices)\", \"Found Node Choices\"\n",
    "]\n",
    "\n",
    "print(f\"Starte die Evaluation von '{input_csv_filename}'...\")\n",
    "with open(input_csv_filename, mode=\"r\", newline=\"\", encoding=\"utf-8\") as infile, \\\n",
    "     open(output_csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as outfile:\n",
    "\n",
    "    reader = csv.DictReader(infile)\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in reader:\n",
    "        question_id = row.get(\"Question ID\", \"\")\n",
    "        question_text = row.get(\"Question\", \"\")\n",
    "        ground_truth = row.get(\"Ground Truth\", \"\").strip().lower()\n",
    "\n",
    "        unshuffled_choices_str = row.get(\"Unshuffled Choices\", \"\")\n",
    "        unshuffled_choices = [c.strip() for c in unshuffled_choices_str.split(',')] if unshuffled_choices_str else []\n",
    "\n",
    "        print(f\"\\n---------- {question_id} ----------\")\n",
    "        print(f\"Question: {question_text}\")\n",
    "\n",
    "        results = {\n",
    "            \"Question ID\": question_id,\n",
    "            \"Question\": question_text,\n",
    "            \"Ground Truth\": row.get(\"Ground Truth\", \"\")\n",
    "        }\n",
    "\n",
    "        # --- Szenario 1: Nur Frage ---\n",
    "        answer_qo, latency_qo, len_thinking_qo, len_answer_qo, _ = evaluate_approach(question_text, add_rag_context=False, add_choices=False)\n",
    "        correctness_qo = \"Correct\" if ground_truth in answer_qo.strip().lower() else \"Incorrect\"\n",
    "        results.update({\n",
    "            \"Answer LLM (Question Only)\": answer_qo,\n",
    "            \"Duration (QO)\": latency_qo,\n",
    "            \"Length Thinking (QO)\": len_thinking_qo,\n",
    "            \"Length Answer (QO)\": len_answer_qo,\n",
    "            \"Correctness (QO)\": correctness_qo\n",
    "        })\n",
    "        print(f\"  [QO] Answer: '{answer_qo}' | Correct: {correctness_qo}\")\n",
    "\n",
    "        # --- Szenario 2: Frage + RAG Graph Wissen ---\n",
    "        answer_rag, latency_rag, len_thinking_rag, len_answer_rag, found_node_rag_info = evaluate_approach(question_text, add_rag_context=True, add_choices=False)\n",
    "        correctness_rag = \"Correct\" if ground_truth in answer_rag.strip().lower() else \"Incorrect\"\n",
    "        results.update({\n",
    "            \"Answer LLM (RAG)\": answer_rag,\n",
    "            \"Duration (RAG)\": latency_rag,\n",
    "            \"Length Thinking (RAG)\": len_thinking_rag,\n",
    "            \"Length Answer (RAG)\": len_answer_rag,\n",
    "            \"Correctness (RAG)\": correctness_rag,\n",
    "            \"Found Node RAG\": found_node_rag_info\n",
    "        })\n",
    "        print(f\"  [RAG] {found_node_rag_info}\")\n",
    "        print(f\"  [RAG] Answer: '{answer_rag}' | Correct: {correctness_rag}\")\n",
    "\n",
    "        # --- Szenario 3: Frage + RAG Graph Wissen + Antwortmöglichkeiten ---\n",
    "        answer_choices, latency_choices, len_thinking_choices, len_answer_choices, found_node_choices_info = evaluate_approach(question_text, add_rag_context=True, add_choices=True, choices=unshuffled_choices)\n",
    "        correctness_choices = \"Correct\" if ground_truth in answer_choices.strip().lower() else \"Incorrect\"\n",
    "        results.update({\n",
    "            \"Answer LLM (Choices)\": answer_choices,\n",
    "            \"Duration (Choices)\": latency_choices,\n",
    "            \"Length Thinking (Choices)\": len_thinking_choices,\n",
    "            \"Length Answer (Choices)\": len_answer_choices,\n",
    "            \"Correctness (Choices)\": correctness_choices,\n",
    "            \"Found Node Choices\": found_node_choices_info\n",
    "        })\n",
    "        print(f\"  [Choices] {found_node_choices_info}\")\n",
    "        print(f\"  [Choices] Answer: '{answer_choices}' | Correct: {correctness_choices}\")\n",
    "\n",
    "        writer.writerow(results)\n",
    "\n",
    "print(f\"\\nEvaluation abgeschlossen. Ergebnisse gespeichert in '{output_csv_filename}'\")"
   ],
   "id": "1a8e7a41d1549b70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " Bestes Skript bisher #####################################################",
   "id": "b41134bef1cca69c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import time\n",
    "import ast\n",
    "\n",
    "# from neo4j import GraphDatabase\n",
    "# from neo4j_graphrag.embeddings import OllamaEmbeddings\n",
    "# from neo4j_graphrag.retrievers import VectorRetriever\n",
    "# from neo4j_graphrag.llm import OllamaLLM\n",
    "\n",
    "# from os import getenv\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv(\".env\")\n",
    "# db_uri = getenv(\"db_uri\")\n",
    "# db_name = getenv(\"db_name\")\n",
    "# db_username = getenv(\"db_username\")\n",
    "# db_password = getenv(\"db_password\")\n",
    "# auth = (db_username, db_password)\n",
    "# driver = GraphDatabase.driver(uri=db_uri, auth=auth)\n",
    "# embedder = OllamaEmbeddings(model=\"nomic-embed-text\") # Oder Ihr spezifisches Modell\n",
    "# llm = OllamaLLM(model_name=\"deepseek-r1:1.5b\") # Oder Ihr spezifisches Modell\n",
    "\n",
    "\n",
    "def get_neighborhood(driver, node_id):\n",
    "\n",
    "    with (driver.session(database=db_name) as session):\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (n)-[r]-(m)\n",
    "            WHERE n.id = $id\n",
    "            RETURN DISTINCT m, type(r) AS rel_type, r.description AS rel_desc\n",
    "        \"\"\", id=node_id)\n",
    "        # Return the list of neighbors and their count\n",
    "        neighbors = [(record[\"m\"], record[\"rel_type\"], record[\"rel_desc\"]) for record in result]\n",
    "        return neighbors, len(neighbors)\n",
    "\n",
    "def build_question_context(main_node, neighbors):\n",
    "    parts = []\n",
    "    parts.append(\"Best similarity search (the main node) is: \" + f\"\"\"\"{main_node.get('name')}\" of type \"{main_node.get('type')}\". Description of \"{main_node.get('name')}\": {main_node.get('description')}\"\"\")\n",
    "    parts.append(\"\\nThe main node's neighbors are the following nodes:\")\n",
    "\n",
    "    for neighbor, rel_type, rel_desc in neighbors:\n",
    "        parts.append(f\"\"\"\n",
    "+++++ {neighbor.get('name').upper()} +++++\n",
    "Node \"{neighbor.get('name')}\" of type \"{neighbor.get('type')}\". Description of \"{neighbor.get('name')}\": {neighbor.get('description')}\n",
    "The main node is related to \"{neighbor.get('name')}\" via type \"{rel_type}\". This relationship contains the following description: {rel_desc}\"\"\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "def evaluate_approach(query_text, add_rag_context=False, add_choices=False, choices=None):\n",
    "\n",
    "    question_context_for_llm_sys_instruction = \"\"\n",
    "    main_retrieved_node_name = \"N/A\"\n",
    "    num_retrieved_neighbors = 0\n",
    "    choices_sent_to_llm = \"N/A\"\n",
    "\n",
    "    if add_rag_context:\n",
    "        retriever = VectorRetriever(driver, \"nodes\", embedder, neo4j_database=db_name)\n",
    "        result = retriever.search(query_text=query_text, top_k=1)\n",
    "\n",
    "        if result.items:\n",
    "            for item in result.items:\n",
    "                dict_item = ast.literal_eval(item.content)\n",
    "                main_retrieved_node_name = dict_item.get('name', 'N/A')\n",
    "\n",
    "                neighbors_of_main_item, num_retrieved_neighbors = get_neighborhood(driver, dict_item[\"id\"])\n",
    "                question_context_for_llm_sys_instruction += build_question_context(dict_item, neighbors_of_main_item)\n",
    "        else:\n",
    "            main_retrieved_node_name = f\"Warning: No relevant nodes found for query: '{query_text}'\"\n",
    "            num_retrieved_neighbors = 0\n",
    "\n",
    "    full_query_to_llm = query_text\n",
    "\n",
    "    if add_choices and choices:\n",
    "        choices_str = \", \".join(choices)\n",
    "        full_query_to_llm += f\"\\n\\nChoose from the following options: {choices_str}\"\n",
    "        choices_sent_to_llm = choices_str\n",
    "\n",
    "    system_instruction = \"\"\n",
    "    if add_rag_context:\n",
    "        system_instruction = question_context_for_llm_sys_instruction\n",
    "        system_instruction += \"\\n\\nAs an IT security expert, based on the provided context and the question, please provide the most accurate answer in English. Start your answer after </think>.\"\n",
    "    else:\n",
    "        system_instruction = \"You are an IT security expert. Please answer the following question in English. Start your answer after </think>.\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = llm.invoke(\n",
    "        input=full_query_to_llm,\n",
    "        system_instruction=system_instruction\n",
    "    )\n",
    "    latency = round(time.time() - start_time, 4)\n",
    "\n",
    "    full_response_content = response.content\n",
    "\n",
    "    thinking_part = \"\"\n",
    "    answer_part = full_response_content\n",
    "    if \"</think>\" in full_response_content:\n",
    "        parts = full_response_content.split(\"</think>\", 1)\n",
    "        thinking_part = parts[0].strip()\n",
    "        answer_part = parts[1].strip()\n",
    "\n",
    "    len_thinking = len(thinking_part)\n",
    "    len_answer = len(answer_part)\n",
    "\n",
    "    display_system_instruction_for_csv = \"As an IT security expert, based on the provided context and the question, please provide the most accurate answer in English. Start your answer after </think>.\"\n",
    "    if not add_rag_context:\n",
    "        display_system_instruction_for_csv = \"You are an IT security expert. Please answer the following question in English. Start your answer after </think>.\"\n",
    "\n",
    "    full_llm_input_combined = f\"System Instruction (simplified for CSV):\\n{display_system_instruction_for_csv}\\n\\nUser Input:\\n{full_query_to_llm}\"\n",
    "\n",
    "\n",
    "    return answer_part, latency, len_thinking, len_answer, full_llm_input_combined, main_retrieved_node_name, num_retrieved_neighbors, choices_sent_to_llm\n",
    "\n",
    "\n",
    "# --- Evaluationsskript ---\n",
    "\n",
    "input_csv_filename = \"AttackSeq-Technique_100.csv\" #best\n",
    "output_csv_filename = \"evaluation_results.csv\"\n",
    "\n",
    "fieldnames = [\n",
    "    \"Question ID\",\n",
    "    \"Question\",\n",
    "    \"Ground Truth\",\n",
    "    \"Answer LLM (QO)\", \"Duration (QO)\", \"Length Thinking (QO)\", \"Length Answer (QO)\", \"Correctness (QO)\", \"LLM Input (QO)\",\n",
    "    \"Answer LLM (RAG)\", \"Duration (RAG)\", \"Length Thinking (RAG)\", \"Length Answer (RAG)\", \"Correctness (RAG)\", \"Main Retrieved Node Name (RAG)\", \"Num Retrieved Neighbors (RAG)\", \"LLM Input (RAG)\",\n",
    "    \"Answer LLM (Choices)\", \"Duration (Choices)\", \"Length Thinking (Choices)\", \"Length Answer (Choices)\", \"Correctness (Choices)\", \"Main Retrieved Node Name (Choices)\", \"Num Retrieved Neighbors (Choices)\", \"LLM Input (Choices)\", \"LLM Choices (Choices)\",\n",
    "    \"Answer LLM (Choices No RAG)\", \"Duration (Choices No RAG)\", \"Length Thinking (Choices No RAG)\", \"Length Answer (Choices No RAG)\", \"Correctness (Choices No RAG)\", \"LLM Input (Choices No RAG)\", \"LLM Choices (Choices No RAG)\"\n",
    "]\n",
    "\n",
    "print(f\"Starte die Evaluation von '{input_csv_filename}'...\")\n",
    "with open(input_csv_filename, mode=\"r\", newline=\"\", encoding=\"utf-8\") as infile, \\\n",
    "     open(output_csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as outfile:\n",
    "\n",
    "    reader = csv.DictReader(infile)\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in reader:\n",
    "        question_id = row.get(\"Question ID\", \"\")\n",
    "        question_text = row.get(\"Question\", \"\")\n",
    "        ground_truth = row.get(\"Ground Truth\", \"\").strip().lower()\n",
    "\n",
    "        unshuffled_choices_str = row.get(\"Unshuffled Choices\", \"\")\n",
    "        unshuffled_choices = [c.strip() for c in unshuffled_choices_str.split(',')] if unshuffled_choices_str else []\n",
    "\n",
    "        print(f\"\\n---------- {question_id} ----------\")\n",
    "        print(f\"Question: {question_text}\")\n",
    "\n",
    "        results = {\n",
    "            \"Question ID\": question_id,\n",
    "            \"Question\": question_text,\n",
    "            \"Ground Truth\": row.get(\"Ground Truth\", \"\")\n",
    "        }\n",
    "\n",
    "        # --- Szenario 1: Nur Frage (Question Only) ---\n",
    "        answer_qo, latency_qo, len_thinking_qo, len_answer_qo, llm_input_qo, _, _, _ = evaluate_approach(question_text, add_rag_context=False, add_choices=False)\n",
    "        correctness_qo = \"Correct\" if ground_truth in answer_qo.strip().lower() else \"Incorrect\"\n",
    "        results.update({\n",
    "            \"Answer LLM (QO)\": answer_qo,\n",
    "            \"Duration (QO)\": latency_qo,\n",
    "            \"Length Thinking (QO)\": len_thinking_qo,\n",
    "            \"Length Answer (QO)\": len_answer_qo,\n",
    "            \"Correctness (QO)\": correctness_qo,\n",
    "            \"LLM Input (QO)\": llm_input_qo\n",
    "        })\n",
    "        print(f\"  [QO] Answer: '{answer_qo}' | Correct: {correctness_qo}\")\n",
    "\n",
    "        # --- Szenario 2: Frage + RAG Graph Wissen (RAG) ---\n",
    "        answer_rag, latency_rag, len_thinking_rag, len_answer_rag, llm_input_rag, main_retrieved_node_name_rag, num_retrieved_neighbors_rag, _ = evaluate_approach(question_text, add_rag_context=True, add_choices=False)\n",
    "        correctness_rag = \"Correct\" if ground_truth in answer_rag.strip().lower() else \"Incorrect\"\n",
    "        results.update({\n",
    "            \"Answer LLM (RAG)\": answer_rag,\n",
    "            \"Duration (RAG)\": latency_rag,\n",
    "            \"Length Thinking (RAG)\": len_thinking_rag,\n",
    "            \"Length Answer (RAG)\": len_answer_rag,\n",
    "            \"Correctness (RAG)\": correctness_rag,\n",
    "            \"Main Retrieved Node Name (RAG)\": main_retrieved_node_name_rag,\n",
    "            \"Num Retrieved Neighbors (RAG)\": num_retrieved_neighbors_rag,\n",
    "            \"LLM Input (RAG)\": llm_input_rag\n",
    "        })\n",
    "        print(f\"  [RAG] Main Node: '{main_retrieved_node_name_rag}', Neighbors: {num_retrieved_neighbors_rag}\")\n",
    "        print(f\"  [RAG] Answer: '{answer_rag}' | Correct: {correctness_rag}\")\n",
    "\n",
    "        # --- Szenario 3: Frage + RAG Graph Wissen + Antwortmöglichkeiten (Choices) ---\n",
    "        answer_choices, latency_choices, len_thinking_choices, len_answer_choices, llm_input_choices, main_retrieved_node_name_choices, num_retrieved_neighbors_choices, choices_sent_to_llm_data = evaluate_approach(question_text, add_rag_context=True, add_choices=True, choices=unshuffled_choices)\n",
    "        correctness_choices = \"Correct\" if ground_truth in answer_choices.strip().lower() else \"Incorrect\"\n",
    "        results.update({\n",
    "            \"Answer LLM (Choices)\": answer_choices,\n",
    "            \"Duration (Choices)\": latency_choices,\n",
    "            \"Length Thinking (Choices)\": len_thinking_choices,\n",
    "            \"Length Answer (Choices)\": len_answer_choices,\n",
    "            \"Correctness (Choices)\": correctness_choices,\n",
    "            \"Main Retrieved Node Name (Choices)\": main_retrieved_node_name_choices,\n",
    "            \"Num Retrieved Neighbors (Choices)\": num_retrieved_neighbors_choices,\n",
    "            \"LLM Input (Choices)\": llm_input_choices,\n",
    "            \"LLM Choices (Choices)\": choices_sent_to_llm_data\n",
    "        })\n",
    "        print(f\"  [Choices] Main Node: '{main_retrieved_node_name_choices}', Neighbors: {num_retrieved_neighbors_choices}\")\n",
    "        print(f\"  [Choices] Choices Sent: '{choices_sent_to_llm_data}'\")\n",
    "        print(f\"  [Choices] Answer: '{answer_choices}' | Correct: {correctness_choices}\")\n",
    "\n",
    "        # --- Szenario 4: Frage + Antwortmöglichkeiten OHNE RAG (Choices No RAG) ---\n",
    "        answer_choices_no_rag, latency_choices_no_rag, len_thinking_choices_no_rag, len_answer_choices_no_rag, llm_input_choices_no_rag, _, _, choices_sent_to_llm_no_rag_data = evaluate_approach(question_text, add_rag_context=False, add_choices=True, choices=unshuffled_choices)\n",
    "        correctness_choices_no_rag = \"Correct\" if ground_truth in answer_choices_no_rag.strip().lower() else \"Incorrect\"\n",
    "        results.update({\n",
    "            \"Answer LLM (Choices No RAG)\": answer_choices_no_rag,\n",
    "            \"Duration (Choices No RAG)\": latency_choices_no_rag,\n",
    "            \"Length Thinking (Choices No RAG)\": len_thinking_choices_no_rag,\n",
    "            \"Length Answer (Choices No RAG)\": len_answer_choices_no_rag,\n",
    "            \"Correctness (Choices No RAG)\": correctness_choices_no_rag,\n",
    "            \"LLM Input (Choices No RAG)\": llm_input_choices_no_rag,\n",
    "            \"LLM Choices (Choices No RAG)\": choices_sent_to_llm_no_rag_data\n",
    "        })\n",
    "        print(f\"  [Choices No RAG] Choices Sent: '{choices_sent_to_llm_no_rag_data}'\")\n",
    "        print(f\"  [Choices No RAG] Answer: '{answer_choices_no_rag}' | Correct: {correctness_choices_no_rag}\")\n",
    "\n",
    "        writer.writerow(results)\n",
    "\n",
    "print(f\"\\nEvaluation complete. Results saved to '{output_csv_filename}'\")"
   ],
   "id": "d2656bb67eae8533",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#####################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################################"
   ],
   "id": "eaa22fbf19fcce12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import csv\n",
    "import time\n",
    "import ast # Needed for ast.literal_eval\n",
    "\n",
    "# from neo4j import GraphDatabase\n",
    "# from neo4j_graphrag.embeddings import OllamaEmbeddings\n",
    "# from neo4j_graphrag.retrievers import VectorRetriever\n",
    "# from neo4j_graphrag.llm import OllamaLLM\n",
    "\n",
    "# from os import getenv\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv(\".env\")\n",
    "# db_uri = getenv(\"db_uri\")\n",
    "# db_name = getenv(\"db_name\")\n",
    "# db_username = getenv(\"db_username\")\n",
    "# db_password = getenv(\"db_password\")\n",
    "# auth = (db_username, db_password)\n",
    "# driver = GraphDatabase.driver(uri=db_uri, auth=auth)\n",
    "# embedder = OllamaEmbeddings(model=\"nomic-embed-text\") # Oder Ihr spezifisches Modell\n",
    "# llm = OllamaLLM(model_name=\"deepseek-r1:1.5b\") # Oder Ihr spezifisches Modell\n",
    "\n",
    "\n",
    "def get_neighborhood(driver, node_id):\n",
    "\n",
    "    with (driver.session(database=db_name) as session):\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (n)-[r]-(m)\n",
    "            WHERE n.id = $id\n",
    "            RETURN DISTINCT m, type(r) AS rel_type, r.description AS rel_desc\n",
    "        \"\"\", id=node_id)\n",
    "        return [(record[\"m\"], record[\"rel_type\"], record[\"rel_desc\"]) for record in result]\n",
    "\n",
    "def build_question_context(main_node, neighbors):\n",
    "\n",
    "    parts = []\n",
    "    parts.append(\"Best similarity search (the main node) is: \" + f\"\"\"\"{main_node.get('name')}\" of type \"{main_node.get('type')}\". Description of \"{main_node.get('name')}\": {main_node.get('description')}\"\"\")\n",
    "    parts.append(\"\\nThe main node's neighbors are the following nodes:\")\n",
    "\n",
    "    for neighbor, rel_type, rel_desc in neighbors:\n",
    "        parts.append(f\"\"\"\n",
    "+++++ {neighbor.get('name').upper()} +++++\n",
    "Node \"{neighbor.get('name')}\" of type \"{neighbor.get('type')}\". Description of \"{neighbor.get('name')}\": {neighbor.get('description')}\n",
    "The main node is related to \"{neighbor.get('name')}\" via type \"{rel_type}\". This relationship contains the following description: {rel_desc}\"\"\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "def evaluate_approach(query_text, add_rag_context=False, add_choices=False, choices=None):\n",
    "\n",
    "    question_context = \"\"\n",
    "    found_node_info = \"N/A\"\n",
    "\n",
    "    if add_rag_context:\n",
    "        retriever = VectorRetriever(driver, \"nodes\", embedder, neo4j_database=db_name)\n",
    "        result = retriever.search(query_text=query_text, top_k=1)\n",
    "\n",
    "        if result.items:\n",
    "            for item in result.items:\n",
    "                dict_item = ast.literal_eval(item.content)\n",
    "                found_node_info = f\"Name='{dict_item.get('name')}', Type='{dict_item.get('type')}', ID='{dict_item.get('id')}'\"\n",
    "\n",
    "                neighbors_of_main_item = get_neighborhood(driver, dict_item[\"id\"])\n",
    "                question_context += build_question_context(dict_item, neighbors_of_main_item)\n",
    "        else:\n",
    "            found_node_info = f\"Warning: No relevant nodes found for query: '{query_text}'\"\n",
    "\n",
    "    full_query_to_llm = query_text\n",
    "\n",
    "    if add_choices and choices:\n",
    "        choices_str = \", \".join(choices)\n",
    "        full_query_to_llm += f\"\\n\\nChoose from the following options: {choices_str}\"\n",
    "\n",
    "    system_instruction = \"\"\n",
    "    if add_rag_context:\n",
    "        system_instruction = question_context\n",
    "        system_instruction += \"\\n\\nAs an IT security expert, based on the provided context and the question, please provide the most accurate answer in English. Start your answer after </think>.\"\n",
    "    else:\n",
    "        system_instruction = \"You are an IT security expert. Please answer the following question in English. Start your answer after </think>.\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = llm.invoke(\n",
    "        input=full_query_to_llm,\n",
    "        system_instruction=system_instruction\n",
    "    )\n",
    "    latency = round(time.time() - start_time, 4)\n",
    "\n",
    "    full_response_content = response.content\n",
    "\n",
    "    thinking_part = \"\"\n",
    "    answer_part = full_response_content\n",
    "    if \"</think>\" in full_response_content:\n",
    "        parts = full_response_content.split(\"</think>\", 1)\n",
    "        thinking_part = parts[0].strip()\n",
    "        answer_part = parts[1].strip()\n",
    "\n",
    "    len_thinking = len(thinking_part)\n",
    "    len_answer = len(answer_part)\n",
    "\n",
    "    full_llm_input = f\"System Instruction:\\n{system_instruction}\\n\\nUser Input:\\n{full_query_to_llm}\"\n",
    "\n",
    "    return answer_part, latency, len_thinking, len_answer, full_llm_input, found_node_info\n",
    "\n",
    "\n",
    "# --- Evaluationsskript ---\n",
    "\n",
    "input_csv_filename = \"../AttackSeq-Technique-Test.csv\"\n",
    "output_csv_filename = \"evaluation_results.csv\"\n",
    "\n",
    "fieldnames = [\n",
    "    \"Question ID\",\n",
    "    \"Question\",\n",
    "    \"Retrieved Nodes (RAG)\",\n",
    "    \"Ground Truth\",\n",
    "    \"Answer LLM (Question Only)\", \"Duration (QO)\", \"Length Thinking (QO)\", \"Length Answer (QO)\", \"Correctness (QO)\", \"LLM Input (QO)\",\n",
    "    \"Answer LLM (RAG)\", \"Duration (RAG)\", \"Length Thinking (RAG)\", \"Length Answer (RAG)\", \"Correctness (RAG)\", \"Found Node RAG\", \"LLM Input (RAG)\",\n",
    "    \"Answer LLM (Choices)\", \"Duration (Choices)\", \"Length Thinking (Choices)\", \"Length Answer (Choices)\", \"Correctness (Choices)\", \"Found Node Choices\", \"LLM Input (Choices)\"\n",
    "]\n",
    "\n",
    "print(f\"Starte die Evaluation von '{input_csv_filename}'...\")\n",
    "with open(input_csv_filename, mode=\"r\", newline=\"\", encoding=\"utf-8\") as infile, \\\n",
    "     open(output_csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as outfile:\n",
    "\n",
    "    reader = csv.DictReader(infile)\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in reader:\n",
    "        question_id = row.get(\"Question ID\", \"\")\n",
    "        question_text = row.get(\"Question\", \"\")\n",
    "        ground_truth = row.get(\"Ground Truth\", \"\").strip().lower()\n",
    "\n",
    "        unshuffled_choices_str = row.get(\"Unshuffled Choices\", \"\")\n",
    "        unshuffled_choices = [c.strip() for c in unshuffled_choices_str.split(',')] if unshuffled_choices_str else []\n",
    "\n",
    "        print(f\"\\n---------- {question_id} ----------\")\n",
    "        print(f\"Question: {question_text}\")\n",
    "\n",
    "        results = {\n",
    "            \"Question ID\": question_id,\n",
    "            \"Question\": question_text,\n",
    "            \"Ground Truth\": row.get(\"Ground Truth\", \"\")\n",
    "        }\n",
    "\n",
    "        # --- Szenario 1: Nur Frage ---\n",
    "        answer_qo, latency_qo, len_thinking_qo, len_answer_qo, llm_input_qo, _ = evaluate_approach(question_text, add_rag_context=False, add_choices=False)\n",
    "        correctness_qo = \"Correct\" if ground_truth in answer_qo.strip().lower() else \"Incorrect\"\n",
    "        results.update({\n",
    "            \"Answer LLM (Question Only)\": answer_qo,\n",
    "            \"Duration (QO)\": latency_qo,\n",
    "            \"Length Thinking (QO)\": len_thinking_qo,\n",
    "            \"Length Answer (QO)\": len_answer_qo,\n",
    "            \"Correctness (QO)\": correctness_qo,\n",
    "            \"LLM Input (QO)\": llm_input_qo\n",
    "        })\n",
    "        print(f\"  [QO] Answer: '{answer_qo}' | Correct: {correctness_qo}\")\n",
    "\n",
    "        # --- Szenario 2: Frage + RAG Graph Wissen ---\n",
    "        answer_rag, latency_rag, len_thinking_rag, len_answer_rag, llm_input_rag, found_node_rag_info = evaluate_approach(question_text, add_rag_context=True, add_choices=False)\n",
    "        correctness_rag = \"Correct\" if ground_truth in answer_rag.strip().lower() else \"Incorrect\"\n",
    "        results.update({\n",
    "            \"Answer LLM (RAG)\": answer_rag,\n",
    "            \"Duration (RAG)\": latency_rag,\n",
    "            \"Length Thinking (RAG)\": len_thinking_rag,\n",
    "            \"Length Answer (RAG)\": len_answer_rag,\n",
    "            \"Correctness (RAG)\": correctness_rag,\n",
    "            \"Found Node RAG\": found_node_rag_info,\n",
    "            \"LLM Input (RAG)\": llm_input_rag\n",
    "        })\n",
    "        results[\"Retrieved Nodes (RAG)\"] = found_node_rag_info\n",
    "        print(f\"  [RAG] **Retrieved Nodes**: {found_node_rag_info}\")\n",
    "        print(f\"  [RAG] Answer: '{answer_rag}' | Correct: {correctness_rag}\")\n",
    "\n",
    "        # --- Szenario 3: Frage + RAG Graph Wissen + Antwortmöglichkeiten ---\n",
    "        answer_choices, latency_choices, len_thinking_choices, len_answer_choices, llm_input_choices, found_node_choices_info = evaluate_approach(question_text, add_rag_context=True, add_choices=True, choices=unshuffled_choices)\n",
    "        correctness_choices = \"Correct\" if ground_truth in answer_choices.strip().lower() else \"Incorrect\"\n",
    "        results.update({\n",
    "            \"Answer LLM (Choices)\": answer_choices,\n",
    "            \"Duration (Choices)\": latency_choices,\n",
    "            \"Length Thinking (Choices)\": len_thinking_choices,\n",
    "            \"Length Answer (Choices)\": len_answer_choices,\n",
    "            \"Correctness (Choices)\": correctness_choices,\n",
    "            \"Found Node Choices\": found_node_choices_info,\n",
    "            \"LLM Input (Choices)\": llm_input_choices\n",
    "        })\n",
    "\n",
    "        print(f\"  [Choices] **Retrieved Nodes**: {found_node_choices_info}\") # Output retrieved node to console\n",
    "        print(f\"  [Choices] Answer: '{answer_choices}' | Correct: {correctness_choices}\")\n",
    "\n",
    "        writer.writerow(results)\n",
    "\n",
    "print(f\"\\nEvaluation complete. Results saved to '{output_csv_filename}'\")"
   ],
   "id": "5af24fc94fb8a682"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
